import json
import base64
import websocket
import sounddevice as sd
import numpy as np
import threading
import queue
import time
import jwt, os
import sys
import pyttsx3
import wave
from io import BytesIO
from memory.data_logger import DialogueLogger
from memory.realtime_sync import create_sync_worker  # ğŸ”‘ æ–¹æ¡ˆ3ï¼šå®æ—¶åŒæ­¥
from dotenv import load_dotenv
from pynput import keyboard  # ç”¨äºé”®ç›˜ç›‘å¬
from .audio_processing import SimpleMyVoiceProcessor

# Load environment variables from .env file
load_dotenv('/Users/xwj/Desktop/gpt-realtime-demo/.env')

# --- å…¨å±€å˜é‡åŒºåŸŸæ–°å¢ ---
tts_queue = queue.Queue() #åˆ›å»ºä¸€ä¸ªä¸“é—¨æ”¾æ–‡å­—çš„é˜Ÿåˆ—

# --- æ–°å¢ï¼šä¸“é—¨çš„ TTS å·¥ä½œçº¿ç¨‹å‡½æ•° ---
def tts_worker_thread():
    """
    åå°çº¿ç¨‹ï¼šä¸“é—¨è´Ÿè´£ä»é˜Ÿåˆ—é‡Œå–æ–‡å­—å¹¶æœ—è¯»
    è¿™æ ·åšä¸ä¼šå¡ä½ WebSocket çš„æ¥æ”¶çº¿ç¨‹
    """
    # åœ¨çº¿ç¨‹å†…éƒ¨åˆå§‹åŒ–å¼•æ“ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨
    engine = pyttsx3.init()
    engine.setProperty('rate', 180)
    engine.setProperty('volume', 5.0) # æ³¨æ„ï¼špyttsx3 éŸ³é‡é€šå¸¸æ˜¯ 0.0-1.0
    
    print("ğŸ”Š Local TTS Worker Started")
    
    while not stop_event.is_set():
        try:
            # ç­‰å¾…é˜Ÿåˆ—ä¸­æœ‰æ–‡å­—ï¼Œè¶…æ—¶1ç§’ä»¥ä¾¿æ£€æŸ¥ stop_event
            text = tts_queue.get(timeout=1.0)
            
            if text:
                print(f"ğŸ—£ï¸  Local TTS Speaking: {text[:20]}...")
                engine.say(text)
                engine.runAndWait() # è¿™é‡Œé˜»å¡åªå½±å“è¿™ä¸ªå­çº¿ç¨‹ï¼Œä¸å½±å“ä¸»ç¨‹åº
                
            tts_queue.task_done()
            
        except queue.Empty:
            continue
        except Exception as e:
            print(f"âŒ TTS Error: {e}")

# å¯é€‰ï¼šè®¾ç½®å£°éŸ³ç±»å‹ï¼ˆæ ¹æ®å¹³å°ä¸åŒæœ‰æ‰€å˜åŒ–ï¼‰
# voices = tts_engine.getProperty('voices')
# tts_engine.setProperty('voice', voices[0].id)

def speak_local_tts(text: str):
    """éé˜»å¡ï¼šå°†æ–‡å­—æ”¾å…¥é˜Ÿåˆ—ï¼Œç”±åå°çº¿ç¨‹æ’­æ”¾"""
    if text and text.strip():
        tts_queue.put(text)

# --- å…¨å±€å˜é‡ ---
API_KEY = os.getenv("ZHIPU_API_KEY")
WS_URL = "wss://open.bigmodel.cn/api/paas/v4/realtime?model=GLM-Realtime"
logger = DialogueLogger(filename="data/save_data.jsonl")

# ğŸ”‘ æ–¹æ¡ˆ3ï¼šå®æ—¶åŒæ­¥å·¥ä½œå™¨
CURRENT_USER_ID = os.getenv("USER_ID", "3f6c7b1a-9d2e-4f8a-b5c3-e1f2a3b4c5d6")
sync_worker = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…å¯åŠ¨æ—¶è¿æ¥å¤±è´¥å½±å“ä¸»æµç¨‹

SAMPLE_RATE = 16000
CHUNK = 1024
CHUNK_DURATION = CHUNK / SAMPLE_RATE  # 0.064 ç§’

# æœ¬åœ°è¯­éŸ³å¤„ç†å™¨ï¼ˆVAD + é¢„ç•™é™å™ªï¼‰
voice_processor = SimpleMyVoiceProcessor(sample_rate=SAMPLE_RATE)

audio_queue = queue.Queue()
session_ready = threading.Event()
stop_event = threading.Event()

# çŠ¶æ€è¿½è¸ª
last_audio_time = time.time()
is_speaking = False

# éŸ³é¢‘æ’­æ”¾ç¼“å†²
audio_playback_buffer = []
playback_lock = threading.Lock()
audio_played_in_response = False  # æ ‡è®°å½“å‰å“åº”æ˜¯å¦å·²æ’­æ”¾éŸ³é¢‘

# ğŸ”‘ AI å›å¤çŠ¶æ€ï¼ˆç”¨äºæ‰“æ–­åŠŸèƒ½ï¼‰
ai_is_responding = False
ai_response_lock = threading.Lock()
ws_global = None  # å…¨å±€ WebSocket å¯¹è±¡ï¼Œç”¨äºæ‰“æ–­åŠŸèƒ½

# --- æ‰“æ–­åŠŸèƒ½ç›¸å…³å‡½æ•° ---

def interrupt_ai_response():
    """æ‰“æ–­ AI çš„å›å¤"""
    global ai_is_responding, audio_playback_buffer
    
    with ai_response_lock:
        if not ai_is_responding:
            print("ğŸ’¡ AI å½“å‰æœªåœ¨å›å¤ï¼Œæ— éœ€æ‰“æ–­")
            return
        
        print("\n" + "="*50)
        print("âš¡ æ£€æµ‹åˆ°æ‰“æ–­ä¿¡å·ï¼æ­£åœ¨åœæ­¢ AI å›å¤...")
        print("="*50)
        
        try:
            # 1. æ ‡è®°åœæ­¢
            ai_is_responding = False
            
            # 2. æ¸…ç©ºéŸ³é¢‘æ’­æ”¾ç¼“å†²
            with playback_lock:
                audio_playback_buffer.clear()
                print("   âœ“ æ¸…ç©ºéŸ³é¢‘ç¼“å†²")
            
            # 3. æ¸…ç©º TTS é˜Ÿåˆ—
            while not tts_queue.empty():
                try:
                    tts_queue.get_nowait()
                except queue.Empty:
                    break
            print("   âœ“ æ¸…ç©º TTS é˜Ÿåˆ—")
            
            # 4. å‘é€å–æ¶ˆå“åº”æ¶ˆæ¯ç»™ GLM
            if ws_global:
                try:
                    ws_global.send(json.dumps({"type": "response.cancel"}))
                    print("   âœ“ å·²å‘é€å–æ¶ˆå“åº”å‘½ä»¤")
                    
                    # æ¸…ç©ºè¾“å…¥éŸ³é¢‘ç¼“å†²
                    time.sleep(0.1)
                    ws_global.send(json.dumps({"type": "input_audio_buffer.clear"}))
                    print("   âœ“ å·²æ¸…ç©ºè¾“å…¥éŸ³é¢‘ç¼“å†²")
                except Exception as e:
                    print(f"   âš ï¸ å‘é€å–æ¶ˆå‘½ä»¤å¤±è´¥: {e}")
            
            print("âœ… AI å·²åœæ­¢ï¼Œæ‚¨å¯ä»¥ç»§ç»­è¯´è¯...")
            print("="*50 + "\n")
            
        except Exception as e:
            print(f"âŒ æ‰“æ–­å¤„ç†å‡ºé”™: {e}")


def keyboard_listener_thread():
    """é”®ç›˜ç›‘å¬çº¿ç¨‹ - ç›‘å¬ Enter é”®ä»¥æ‰“æ–­ AI"""
    
    def on_press(key):
        try:
            # æ£€æµ‹ Enter é”®
            if key == keyboard.Key.enter:
                if ai_is_responding:
                    interrupt_ai_response()
        except AttributeError:
            pass
    
    print("âŒ¨ï¸  é”®ç›˜ç›‘å¬å·²å¯åŠ¨ (æŒ‰ Enter å¯æ‰“æ–­ AI å›å¤)")
    
    # å¯åŠ¨ç›‘å¬å™¨
    with keyboard.Listener(on_press=on_press) as listener:
        listener.join()


# --- æ ¸å¿ƒå‡½æ•° ---

def pcm_to_wav_base64(pcm_data: np.ndarray, sample_rate: int = 16000) -> str:
    """
    å°† PCM éŸ³é¢‘æ•°æ®åŒ…è£…æˆ WAV æ ¼å¼å¹¶è½¬ä¸º base64
    Args:
        pcm_data: int16 æ ¼å¼çš„ numpy æ•°ç»„
        sample_rate: é‡‡æ ·ç‡
    Returns:
        base64 ç¼–ç çš„ WAV æ•°æ®
    """
    wav_io = BytesIO()
    with wave.open(wav_io, "wb") as wav_out:
        wav_out.setnchannels(1)  # å•å£°é“
        wav_out.setsampwidth(2)  # 16bit = 2 bytes
        wav_out.setframerate(sample_rate)
        wav_out.writeframes(pcm_data.tobytes())
    
    wav_io.seek(0)
    return base64.b64encode(wav_io.getvalue()).decode("utf-8")

def generate_jwt_token(api_key: str, exp_seconds: int = 3600) -> str:
    """Generate JWT token for authentication."""
    try:
        api_key_id, api_key_secret = api_key.split('.')
    except ValueError:
        raise ValueError("API Key format is incorrect, should be 'API_KEY_ID.API_KEY_SECRET'")
    current_time = int(time.time())
    payload = {"api_key": api_key_id, "exp": current_time + exp_seconds, "timestamp": current_time}
    encoded_jwt = jwt.encode(payload, api_key_secret, algorithm="HS256",
                             headers={"alg": "HS256", "sign_type": "SIGN"})
    return encoded_jwt

def callback(indata, frames, time_info, status):
    """sounddevice input stream callback function."""
    global last_audio_time, is_speaking
    
    if status:
        print("Microphone Warning:", status, file=sys.stderr)
    
    volume_norm = np.linalg.norm(indata) * 10 
    
    if volume_norm > 0.5:
        print(f"ğŸ”Š Sound Detected (Level: {volume_norm:.1f})", end='\r', file=sys.stdout, flush=True)
        last_audio_time = time.time()
        is_speaking = True

    # å¦‚æœå·²ç»åœ¨åœæ­¢æµç¨‹ä¸­ï¼Œç›´æ¥è¿”å›
    if stop_event.is_set():
        return

    # å…ˆç»è¿‡æœ¬åœ°è¯­éŸ³å¤„ç†å™¨ï¼ˆVAD + é¢„ç•™é™å™ªï¼‰
    processed = voice_processor.process(indata)
    if processed is not None:
        audio_queue.put(processed.copy())

def send_audio_loop(ws):
    """
    ä¼˜åŒ–ç‰ˆéŸ³é¢‘å‘é€ï¼š
    1. ä½¿ç”¨é€Ÿç‡é™åˆ¶å™¨ï¼Œç¡®ä¿ä¸è¶…è¿‡ 50 QPS
    2. æ‰¹é‡ç´¯ç§¯éŸ³é¢‘ï¼Œå‡å°‘è¯·æ±‚æ¬¡æ•°
    3. æ™ºèƒ½æ£€æµ‹é™éŸ³å¹¶è§¦å‘å“åº”
    """
    global is_speaking, last_audio_time
    
    session_ready.wait()
    print("ğŸ¤ Session ready, starting to send audio stream")
    
    # é€Ÿç‡é™åˆ¶é…ç½®
    MAX_QPS = 45  
    MIN_INTERVAL = 1.0 / MAX_QPS  # æ¯æ¬¡è¯·æ±‚æœ€å°é—´éš” â‰ˆ 0.022ç§’
    
    # æ‰¹é‡å‘é€é…ç½®
    BATCH_SIZE = 8  # æ¯æ¬¡ç´¯ç§¯8ä¸ªchunk (8 * 64ms = 512ms éŸ³é¢‘)
    SILENCE_THRESHOLD = 1.5  # é™éŸ³1.5ç§’è§¦å‘å“åº”
    
    audio_batch = []
    last_send_time = 0
    
    while not stop_event.is_set():
        try:
            chunk = audio_queue.get(timeout=0.05)
            audio_batch.append(chunk)
            
            # å½“ç´¯ç§¯åˆ°è¶³å¤Ÿçš„éŸ³é¢‘ ä¸” æ»¡è¶³é€Ÿç‡é™åˆ¶
            if len(audio_batch) >= BATCH_SIZE:
                # ç¡®ä¿æ»¡è¶³æœ€å°é—´éš”
                time_since_last_send = time.time() - last_send_time
                if time_since_last_send < MIN_INTERVAL:
                    time.sleep(MIN_INTERVAL - time_since_last_send)
                
                # å‘é€æ‰¹é‡éŸ³é¢‘ï¼ˆåŒ…è£…æˆ WAV æ ¼å¼ï¼‰
                combined_audio = np.concatenate(audio_batch)
                audio_base64 = pcm_to_wav_base64(combined_audio, SAMPLE_RATE)
                
                ws.send(json.dumps({
                    "type": "input_audio_buffer.append",
                    "audio": audio_base64
                }))
                
                last_send_time = time.time()
                audio_batch.clear()
                
                # æ¸…ç©ºé˜Ÿåˆ—ä¸­çš„å·²å¤„ç†ä»»åŠ¡
                for _ in range(BATCH_SIZE):
                    try:
                        audio_queue.task_done()
                    except:
                        pass

        except queue.Empty:
            # æ£€æµ‹é™éŸ³å¹¶è§¦å‘å“åº”
            if is_speaking and (time.time() - last_audio_time) > SILENCE_THRESHOLD:
                print("\nâ¸ï¸  Detected silence, committing audio and requesting response...")
                
                # å‘é€å‰©ä½™çš„éŸ³é¢‘ï¼ˆåŒ…è£…æˆ WAV æ ¼å¼ï¼‰
                if audio_batch:
                    # æ»¡è¶³é€Ÿç‡é™åˆ¶
                    time_since_last_send = time.time() - last_send_time
                    if time_since_last_send < MIN_INTERVAL:
                        time.sleep(MIN_INTERVAL - time_since_last_send)
                    
                    combined_audio = np.concatenate(audio_batch)
                    audio_base64 = pcm_to_wav_base64(combined_audio, SAMPLE_RATE)
                    ws.send(json.dumps({
                        "type": "input_audio_buffer.append",
                        "audio": audio_base64
                    }))
                    audio_batch.clear()
                    last_send_time = time.time()
                
                # çŸ­æš‚ç­‰å¾…ï¼Œç„¶åcommit
                time.sleep(MIN_INTERVAL)
                ws.send(json.dumps({"type": "input_audio_buffer.commit"}))
                
                # å†ç­‰å¾…ä¸€ä¸‹ï¼Œç„¶åè¯·æ±‚å“åº”
                time.sleep(MIN_INTERVAL)
                response_request = {
                    "type": "response.create",
                    "response": {
                        "modalities": ["audio", "text"],
                        "instructions": "è¯·ç”¨è¯­éŸ³å›å¤"
                    }
                }
                print(f"ğŸ“¤ Sending response request: {json.dumps(response_request, ensure_ascii=False)}")
                ws.send(json.dumps(response_request))
                
                is_speaking = False
                last_send_time = time.time()
                print("ğŸ“¤ Response creation requested")
            
            continue
        
        except Exception as e:
            print(f"\nâŒ Send error: {e}")
            break
            
    print("ğŸ¤ Audio sending thread exited.")


def on_message(ws, message):
    """Handles incoming WebSocket messages."""
    global audio_played_in_response, ai_is_responding, sync_worker  # å£°æ˜å…¨å±€å˜é‡
    
    data = json.loads(message)
    msg_type = data.get("type")
    
    # åªåœ¨å…³é”®æ¶ˆæ¯æ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯
    if msg_type in ("session.created", "session.updated", "error", "session.error"):
        print(f"\nğŸ“¡ [{msg_type}] {json.dumps(data, ensure_ascii=False, indent=2)}")
    
    if msg_type in ("session.created", "session.updated"):
        print("âœ… Session Info:", data.get("session", {}).get("id"))
        session_ready.set()
        
    elif msg_type == "conversation.item.input_audio_transcription.completed":
        # è¿™å°±æ˜¯ä½ çš„è¯­éŸ³è½¬æ–‡æœ¬ï¼
        user_text = data.get("transcript", "")
        if user_text:
            # ä¸ºäº†åŒºåˆ†ï¼Œæˆ‘ä»¬ç»™å®ƒä¸€ä¸ªç‰¹æ®Šçš„æ‰“å°å‰ç¼€
            print(f"\nğŸ“ [USER_TEXT]: {user_text}")
            logger.log_user_input(user_text)
        
    elif msg_type == "response.text.delta":
        text = data.get("text", "")
        if text:
            sys.stdout.write(text)
            sys.stdout.flush()
        
    elif msg_type == "response.text.done":
        sys.stdout.write("\n")
        sys.stdout.flush()
        
    elif msg_type == "response.audio.delta":
        try:
            # ğŸ”‘ æ ‡è®° AI å¼€å§‹å›å¤ï¼ˆç¬¬ä¸€æ¬¡æ¥æ”¶éŸ³é¢‘æ—¶ï¼‰
            if not ai_is_responding:
                with ai_response_lock:
                    ai_is_responding = True
                    print("\nğŸ”Š [AI å›å¤ä¸­] æŒ‰ Enter å¯æ‰“æ–­")
            
            audio_base64 = data.get("delta", "")  # ğŸ”‘ ä¿®å¤ï¼šå­—æ®µåæ˜¯ "delta" ä¸æ˜¯ "audio"
            if not audio_base64:
                print(f"\nâš ï¸  Received audio.delta with empty delta field!")
                return
                
            audio_bytes = base64.b64decode(audio_base64)
            audio_np = np.frombuffer(audio_bytes, dtype=np.int16)
            
            # è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ”Š Audio chunk: {len(audio_bytes)} bytes", end='\r', flush=True)
            
            # ç´¯ç§¯éŸ³é¢‘åˆ°ç¼“å†²åŒº
            with playback_lock:
                audio_playback_buffer.append(audio_np)
            
        except Exception as e:
            print(f"\nâŒ Audio processing error: {e}")
            import traceback
            traceback.print_exc()
            
    elif msg_type == "response.audio.done":
        try:
            print(f"\n\nğŸµ Audio stream complete, preparing playback...")
            
            with playback_lock:
                if audio_playback_buffer:
                    print(f"   Buffered chunks: {len(audio_playback_buffer)}")
                    
                    # åˆå¹¶æ‰€æœ‰éŸ³é¢‘å—
                    full_audio = np.concatenate(audio_playback_buffer)
                    original_duration = len(full_audio)/SAMPLE_RATE
                    print(f"   Total samples: {len(full_audio)}, original duration: {original_duration:.2f}s")
                    
                    # å¢åŠ éŸ³é‡ï¼ˆå¦‚æœå¤ªå°ï¼‰
                    max_val = np.abs(full_audio).max()
                    if max_val > 0:
                        if max_val < 10000:
                            volume_boost = 10000 / max_val
                            full_audio = (full_audio * volume_boost).astype(np.int16)
                            print(f"   ğŸ”Š Volume boosted by {volume_boost:.2f}x")
                        
                        # ğŸš€ åŠ é€Ÿæ’­æ”¾
                        SPEED_MULTIPLIER = 1.5  # è°ƒæ•´æ’­æ”¾é€Ÿåº¦ï¼ˆæ¨è 1.3-1.8ï¼‰
                        playback_rate = int(SAMPLE_RATE * SPEED_MULTIPLIER)
                        adjusted_duration = len(full_audio) / playback_rate
                        
                        print(f"   âš¡ Speed: {SPEED_MULTIPLIER}x, playback duration: {adjusted_duration:.2f}s")
                        print(f"   â–¶ï¸  Playing...")
                        sd.play(full_audio, samplerate=playback_rate, blocking=True)
                        print("   âœ… Playback complete!")
                        
                        # ğŸ”‘ æ ‡è®°å·²æ’­æ”¾éŸ³é¢‘ï¼Œä¸éœ€è¦æœ¬åœ°TTSäº†
                        audio_played_in_response = True
                    else:
                        print("   âš ï¸  Audio data is silent (all zeros)")
                    
                    audio_playback_buffer.clear()
                else:
                    print("   âš ï¸  No audio chunks were buffered!")
                    
        except Exception as e:
            print(f"\nâŒ Error during playback: {e}")
            import traceback
            traceback.print_exc()
            
    elif msg_type == "response.output_item.done":
        # â­ è¿™ä¸ªäº‹ä»¶å¯èƒ½åŒ…å«éŸ³é¢‘æ•°æ®ï¼ˆä½†é€šå¸¸éŸ³é¢‘å·²ç»é€šè¿‡ audio.done æ’­æ”¾äº†ï¼‰
        try:
            item = data.get("item", {})
            content_list = item.get("content", [])
            
            for i, content in enumerate(content_list):
                content_type = content.get("type")
                
                if content_type == "audio":
                    transcript = content.get("transcript", "")
                    
                    # è®°å½•è½¬å†™æ–‡æœ¬
                    if transcript:
                        logger.log_assistant_delta(transcript)
                        print(f"\nğŸ“ AI: {transcript}")
                    
                    # ğŸ”‘ å…³é”®ï¼šåªåœ¨æ²¡æœ‰é€šè¿‡ audio.done æ’­æ”¾éŸ³é¢‘æ—¶æ‰ä½¿ç”¨æœ¬åœ° TTS
                    if not audio_played_in_response and transcript:
                        print(f"\nâš ï¸  No audio received, using local TTS fallback")
                        speak_local_tts(transcript)
                        
        except Exception as e:
            print(f"\nâŒ Error processing output_item: {e}")
            import traceback
            traceback.print_exc()
    
    elif msg_type == "response.done":
        # ğŸ”‘ æ ‡è®° AI å›å¤ç»“æŸ
        with ai_response_lock:
            ai_is_responding = False
        
        print("ğŸ‰ Response complete")
        print("ğŸ¤ [æ­£åœ¨å¬...] æ‚¨å¯ä»¥è¯´è¯äº†\n" + "="*40)
        
        # ğŸ”‘ æ–¹æ¡ˆ3ï¼šå®æ—¶åŒæ­¥åˆ° Memobase
        dialogue_data = logger.finalize_turn()
        
        if dialogue_data and sync_worker:
            # å°è¯•å®æ—¶åŒæ­¥ï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡ï¼‰
            try:
                success = sync_worker.enqueue(dialogue_data)
                if success:
                    print("ğŸ“¤ [å®æ—¶åŒæ­¥] å·²åŠ å…¥åŒæ­¥é˜Ÿåˆ—")
                else:
                    print("âš ï¸  [å®æ—¶åŒæ­¥] åŠ å…¥é˜Ÿåˆ—å¤±è´¥ï¼Œå°†ç”±å®šæ—¶ä»»åŠ¡å¤„ç†")
            except Exception as e:
                print(f"âš ï¸  [å®æ—¶åŒæ­¥] å‡ºé”™: {e}ï¼Œå°†ç”±å®šæ—¶ä»»åŠ¡å¤„ç†")
        elif dialogue_data and not sync_worker:
            print("ğŸ’¡ [å®æ—¶åŒæ­¥] å·¥ä½œå™¨æœªå°±ç»ªï¼Œå°†ç”±å®šæ—¶ä»»åŠ¡å¤„ç†")
        
        # ğŸ”„ é‡ç½®éŸ³é¢‘æ’­æ”¾æ ‡å¿—ï¼Œä¸ºä¸‹ä¸€è½®å¯¹è¯åšå‡†å¤‡
        audio_played_in_response = False
        
    elif msg_type == "input_audio_buffer.committed":
        print("âœ… Audio buffer committed")
        
    elif msg_type == "input_audio_buffer.speech_started":
        print("\nğŸ¤ Speech detected by server VAD")
        
    elif msg_type == "input_audio_buffer.speech_stopped":
        print("â¸ï¸  Speech ended (detected by server VAD)")
        
    elif msg_type in ("session.error", "error"):
        error_info = data.get('error', {})
        error_code = error_info.get('code', '')
        
        # åªæ˜¾ç¤ºéé€Ÿç‡é™åˆ¶çš„é”™è¯¯ï¼Œé€Ÿç‡é™åˆ¶é”™è¯¯å¤ªå¤šä¼šåˆ·å±
        if error_code != 'rate_limit_error':
            print(f"âŒ Error: {error_info.get('message', data)}")
        
    elif msg_type == "heartbeat":
        pass
        
    elif msg_type in ("rate_limits.updated", "conversation.created", "conversation.updated"):
        # é™é»˜å¤„ç†è¿™äº›å¸¸è§æ¶ˆæ¯
        pass
        
    else:
        # åªæ‰“å°çœŸæ­£æœªçŸ¥çš„æ¶ˆæ¯ç±»å‹
# <--- ä¿®æ”¹ç‚¹ï¼šæ‰“å°æ‰€æœ‰æˆ‘ä»¬æœªçŸ¥çš„æ¶ˆæ¯çš„ å®Œæ•´å†…å®¹ï¼ --->
        if not msg_type.startswith(("response.", "input_audio_buffer.")):
            print(f"ğŸ’¡ Message: {msg_type}")


def on_open(ws):
    """Called when WebSocket connection is established."""
    print("ğŸ”Œ WebSocket connected, configuring session...")
    session_config = {
        "type": "session.update",
        "session": {
            "input_audio_format": "wav",
            "output_audio_format": "pcm",
            "turn_detection": {
                "type": "server_vad",
                "threshold": 0.5,              # éŸ³é‡é˜ˆå€¼ (0.0-1.0)
                "prefix_padding_ms": 300,      # è¯´è¯å‰ç¼“å†² (æ¯«ç§’)
                "silence_duration_ms": 2000    # ğŸ”‘ é™é»˜2ç§’æ‰åˆ¤å®šè¯´å®Œ (æ¯«ç§’)
            },
            "input_audio_transcription": {
                "enabled": True
            },
            "temperature": 0.8,  # è‡ªç„¶åº¦
            "modalities": ["audio", "text"],
            "beta_fields": {
               "chat_mode": "audio",
               "tts_source": "e2e",  # ç«¯åˆ°ç«¯è¯­éŸ³åˆæˆ
               "auto_search": False
               # æ³¨æ„ï¼šspeed å‚æ•°ä¸ç”Ÿæ•ˆï¼Œä½¿ç”¨å®¢æˆ·ç«¯æ’­æ”¾åŠ é€Ÿ
           }
        }
    }
    print(f"ğŸ“¤ Session config: {json.dumps(session_config, ensure_ascii=False, indent=2)}")
    ws.send(json.dumps(session_config))
    time.sleep(0.5)
    threading.Thread(target=send_audio_loop, args=(ws,), daemon=True).start()

def on_close(ws, close_status_code, close_msg):
    """Called when WebSocket connection closes."""
    if not stop_event.is_set():
         stop_event.set()
    print(f"\nğŸ”Œ Connection closed: code={close_status_code}, msg={close_msg}")

def on_error(ws, error):
    """Called on WebSocket error."""
    if not stop_event.is_set():
         stop_event.set()
    print(f"âŒ WebSocket error: {error}", file=sys.stderr)

# --- Main Program Logic ---

if __name__ == "__main__":
    if not API_KEY:
        print("âŒ Please set the ZHIPU_API_KEY environment variable first")
        sys.exit(1)

    # å…ˆæ£€æŸ¥éŸ³é¢‘è®¾å¤‡
    print("\n" + "="*50)
    print("ğŸ”Š Audio Device Check")
    print("="*50)
    try:
        devices = sd.query_devices()
        print(f"Default input device: {sd.query_devices(kind='input')['name']}")
        print(f"Default output device: {sd.query_devices(kind='output')['name']}")
        
        # æµ‹è¯•éŸ³é¢‘æ’­æ”¾
        print("\nğŸ§ª Testing audio playback...")
        test_tone = (np.sin(2 * np.pi * 440 * np.arange(SAMPLE_RATE) / SAMPLE_RATE) * 5000).astype(np.int16)
        sd.play(test_tone, samplerate=SAMPLE_RATE, blocking=True)
        print("âœ… If you heard a beep, audio output is working!")
        time.sleep(0.5)
    except Exception as e:
        print(f"âš ï¸  Audio device warning: {e}")
    print("="*50 + "\n")

    # ğŸ”‘ æ–¹æ¡ˆ3ï¼šåˆå§‹åŒ–å®æ—¶åŒæ­¥å·¥ä½œå™¨
    # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦ global å£°æ˜ï¼Œå› ä¸º if __name__ == "__main__" æœ¬èº«å°±åœ¨æ¨¡å—çº§åˆ«
    print("ğŸ”§ åˆå§‹åŒ– Memobase å®æ—¶åŒæ­¥...")
    try:
        sync_worker = create_sync_worker(CURRENT_USER_ID)
        print("âœ… å®æ—¶åŒæ­¥å·¥ä½œå™¨å·²å¯åŠ¨")
    except Exception as e:
        print(f"âš ï¸  å®æ—¶åŒæ­¥å·¥ä½œå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ’¡ å°†ä½¿ç”¨å®šæ—¶ä»»åŠ¡ä½œä¸ºåå¤‡åŒæ­¥æ–¹å¼")
        sync_worker = None

    try:
        AUTH_TOKEN = generate_jwt_token(API_KEY)
        print("âœ… JWT generated successfully")
    except Exception as e:
        print(f"âŒ JWT generation failed: {e}")
        sys.exit(1)

    print("\n" + "="*50)
    print("    GLM-Realtime Voice Chat")
    print("="*50)
    print("ğŸ’¡ Usage:")
    print("   1. Speak into the microphone")
    print("   2. Pause for 2 seconds to get response")
    print("   3. Press Enter to interrupt AI")
    print("   4. Press Ctrl+C to exit")
    print("="*50 + "\n")
    
    threading.Thread(target=tts_worker_thread, daemon=True).start()
    
    # ğŸ”‘ å¯åŠ¨é”®ç›˜ç›‘å¬çº¿ç¨‹ï¼ˆç”¨äºæ‰“æ–­åŠŸèƒ½ï¼‰
    threading.Thread(target=keyboard_listener_thread, daemon=True).start()

    websocket.enableTrace(False)
    
    ws = websocket.WebSocketApp(
        WS_URL,
        header=[f"Authorization: Bearer {AUTH_TOKEN}"],
        on_message=on_message,
        on_open=on_open,
        on_close=on_close,
        on_error=on_error
    )
    
    # ğŸ”‘ è®¾ç½®å…¨å±€ WebSocket å¯¹è±¡ï¼ˆç”¨äºæ‰“æ–­åŠŸèƒ½ï¼‰
    ws_global = ws  # ç›´æ¥èµ‹å€¼ï¼Œä¸éœ€è¦ global å£°æ˜
    
    ws_thread = threading.Thread(target=ws.run_forever, daemon=True)
    ws_thread.start()

    try:
        print("â³ Waiting for connection...")
        session_ready.wait(timeout=10)

        if not session_ready.is_set():
            print("âŒ Session setup timeout")
            sys.exit(1)

        print("ğŸ¤ [æ­£åœ¨å¬...] Ready! Start speaking...")
        print("ğŸ’¡ æç¤º: AI å›å¤æ—¶æŒ‰ Enter é”®å¯æ‰“æ–­å¹¶ç»§ç»­è¯´è¯\n")
        
        with sd.InputStream(channels=1, samplerate=SAMPLE_RATE, dtype='int16', callback=callback):
            ws_thread.join()

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Interrupted by user")
    except Exception as e:
        print(f"\nâŒ Runtime error: {e}")
    finally:
        print("\nğŸ›‘ æ­£åœ¨æ¸…ç†èµ„æº...")
        stop_event.set()
        
        # ğŸ”‘ æ–¹æ¡ˆ3ï¼šåœæ­¢åŒæ­¥å·¥ä½œå™¨
        if sync_worker:
            print("â³ ç­‰å¾…åŒæ­¥é˜Ÿåˆ—æ¸…ç©º...")
            sync_worker.stop(timeout=5)
        
        if ws:
             threading.Thread(target=ws.close).start()
        sd.stop()
        print("âœ… å·²é€€å‡º")