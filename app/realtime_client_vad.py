import json
import base64
import websocket
import sounddevice as sd
import numpy as np
import threading
import queue
import time
import jwt, os
import sys
import pyttsx3
import wave
from io import BytesIO
from memory.data_logger import DialogueLogger
from memory.realtime_sync import create_sync_worker
from dotenv import load_dotenv
from pynput import keyboard
from .audio_processing import SimpleMyVoiceProcessor

# Load environment variables
load_dotenv('/Users/xwj/Desktop/gpt-realtime-demo/.env')

# --- å…¨å±€å˜é‡ ---
tts_queue = queue.Queue()

def tts_worker_thread():
    """åŽå° TTS å·¥ä½œçº¿ç¨‹"""
    engine = pyttsx3.init()
    engine.setProperty('rate', 180)
    engine.setProperty('volume', 0.9)
    
    print("ðŸ”Š Local TTS Worker Started")
    
    while not stop_event.is_set():
        try:
            text = tts_queue.get(timeout=1.0)
            if text:
                print(f"ðŸ—£ï¸  Local TTS Speaking: {text[:20]}...")
                engine.say(text)
                engine.runAndWait()
            tts_queue.task_done()
        except queue.Empty:
            continue
        except Exception as e:
            print(f"âŒ TTS Error: {e}")

def speak_local_tts(text: str):
    """éžé˜»å¡ž TTS"""
    if text and text.strip():
        tts_queue.put(text)

# --- é…ç½® ---
API_KEY = os.getenv("ZHIPU_API_KEY")
WS_URL = "wss://open.bigmodel.cn/api/paas/v4/realtime?model=GLM-Realtime"
logger = DialogueLogger(filename="data/save_data.jsonl")

# å®žæ—¶åŒæ­¥å·¥ä½œå™¨
CURRENT_USER_ID = os.getenv("USER_ID", "3f6c7b1a-9d2e-4f8a-b5c3-e1f2a3b4c5d6")
sync_worker = None

SAMPLE_RATE = 16000
CHUNK = 1024
CHUNK_DURATION = CHUNK / SAMPLE_RATE

# ðŸ”‘ Client VAD é…ç½®
voice_processor = SimpleMyVoiceProcessor(sample_rate=SAMPLE_RATE, vad_aggressiveness=2)

# ðŸ”‘ è¯­éŸ³çŠ¶æ€ç®¡ç†
class SpeechDetector:
    """å®¢æˆ·ç«¯è¯­éŸ³æ£€æµ‹å™¨"""
    def __init__(self, silence_threshold_seconds=1.5):
        self.is_speaking = False
        self.last_speech_time = 0
        self.silence_threshold = silence_threshold_seconds
        self.speech_started = False
    
    def update(self, has_speech):
        """æ›´æ–°è¯­éŸ³çŠ¶æ€"""
        current_time = time.time()
        
        if has_speech:
            if not self.is_speaking:
                self.is_speaking = True
                self.speech_started = True
                print("\nðŸŽ¤ [å¼€å§‹è¯´è¯]")
            self.last_speech_time = current_time
            return "speaking"
        else:
            if self.is_speaking:
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™éŸ³é˜ˆå€¼
                silence_duration = current_time - self.last_speech_time
                if silence_duration > self.silence_threshold:
                    self.is_speaking = False
                    if self.speech_started:
                        self.speech_started = False
                        print(f"\nðŸ”‡ [åœæ­¢è¯´è¯] (é™éŸ³ {silence_duration:.1f}ç§’)")
                        return "speech_end"
            return "silence"

speech_detector = SpeechDetector(silence_threshold_seconds=1.5)

audio_queue = queue.Queue()
session_ready = threading.Event()
stop_event = threading.Event()

# éŸ³é¢‘æ’­æ”¾ç¼“å†²
audio_playback_buffer = []
playback_lock = threading.Lock()
audio_played_in_response = False

# AI å›žå¤çŠ¶æ€
ai_is_responding = False
ai_response_lock = threading.Lock()
ws_global = None

# --- æ‰“æ–­åŠŸèƒ½ ---
def interrupt_ai_response():
    """æ‰“æ–­ AI å›žå¤"""
    global ai_is_responding, ws_global
    
    with ai_response_lock:
        if ai_is_responding and ws_global:
            print("\n\nâš ï¸  [ç”¨æˆ·æ‰“æ–­] æ­£åœ¨å–æ¶ˆ AI å›žå¤...")
            try:
                # æ¸…ç©ºæ’­æ”¾ç¼“å†²
                with playback_lock:
                    audio_playback_buffer.clear()
                
                # åœæ­¢éŸ³é¢‘æ’­æ”¾
                sd.stop()
                
                # å‘é€å–æ¶ˆè¯·æ±‚
                cancel_msg = {
                    "type": "response.cancel"
                }
                ws_global.send(json.dumps(cancel_msg))
                print("   âœ… å·²å‘é€å–æ¶ˆè¯·æ±‚")
                
                ai_is_responding = False
                
                # æ¸…ç©ºéŸ³é¢‘ç¼“å†²
                audio_queue.queue.clear()
                print("   ðŸ”„ å·²æ¸…ç©ºç¼“å†²åŒºï¼Œå¯ä»¥ç»§ç»­è¯´è¯")
                
            except Exception as e:
                print(f"   âŒ æ‰“æ–­å¤±è´¥: {e}")
        else:
            print("\nðŸ’¡ AI å½“å‰æœªåœ¨å›žå¤ä¸­")

def on_press(key):
    """é”®ç›˜ç›‘å¬å›žè°ƒ"""
    try:
        if key == keyboard.Key.enter:
            interrupt_ai_response()
    except Exception as e:
        pass

# å¯åŠ¨é”®ç›˜ç›‘å¬
keyboard_listener = keyboard.Listener(on_press=on_press)
keyboard_listener.start()

# --- JWT Token ç”Ÿæˆ ---
def generate_jwt_token(api_key: str, exp_seconds: int = 3600) -> str:
    try:
        api_key_id, api_key_secret = api_key.split('.')
    except ValueError:
        raise ValueError("Invalid API Key format")
    
    current_time = int(time.time())
    payload = {
        "api_key": api_key_id,
        "exp": current_time + exp_seconds,
        "timestamp": current_time
    }
    encoded_jwt = jwt.encode(
        payload, api_key_secret,
        algorithm="HS256",
        headers={"alg": "HS256", "sign_type": "SIGN"}
    )
    return encoded_jwt

# --- éŸ³é¢‘é‡‡é›†å›žè°ƒ ---
def audio_callback(indata, frames, time_info, status):
    """sounddevice å›žè°ƒå‡½æ•°"""
    if status:
        print(f"Audio callback status: {status}")
    if not stop_event.is_set():
        audio_queue.put(indata.copy())

# --- ðŸ”‘ Client VAD éŸ³é¢‘å‘é€å¾ªçŽ¯ ---
def send_audio_loop(ws):
    """ä½¿ç”¨ Client VAD çš„éŸ³é¢‘å‘é€å¾ªçŽ¯"""
    global ws_global
    ws_global = ws
    
    print("ðŸŽ¤ å¼€å§‹å½•éŸ³ï¼ˆClient VAD æ¨¡å¼ï¼‰...")
    print("ðŸ’¡ æç¤ºï¼šè¯´è¯æ—¶ä¼šè‡ªåŠ¨æ£€æµ‹ï¼Œåœé¡¿ 1.5 ç§’åŽè‡ªåŠ¨æäº¤å¹¶è¯·æ±‚ AI å›žå¤")
    print("ðŸ’¡ æŒ‰ Enter å¯æ‰“æ–­ AI å›žå¤\n")
    
    # ç­‰å¾…ä¼šè¯å°±ç»ª
    session_ready.wait()
    
    while not stop_event.is_set():
        try:
            # èŽ·å–éŸ³é¢‘æ•°æ®ï¼ˆè¶…æ—¶ 0.1 ç§’ï¼‰
            audio_chunk = audio_queue.get(timeout=0.1)
            
            # ðŸ”‘ æœ¬åœ° VAD æ£€æµ‹
            processed = voice_processor.process(audio_chunk)
            has_speech = processed is not None
            
            # æ›´æ–°è¯­éŸ³çŠ¶æ€
            speech_status = speech_detector.update(has_speech)
            
            if has_speech:
                # æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå‘é€åˆ°æœåŠ¡å™¨
                wav_io = BytesIO()
                with wave.open(wav_io, "wb") as wav_file:
                    wav_file.setnchannels(1)
                    wav_file.setsampwidth(2)
                    wav_file.setframerate(SAMPLE_RATE)
                    wav_file.writeframes(processed.tobytes())
                
                wav_io.seek(0)
                audio_base64 = base64.b64encode(wav_io.getvalue()).decode("utf-8")
                
                # å‘é€éŸ³é¢‘æ•°æ®
                message = {
                    "type": "input_audio_buffer.append",
                    "audio": audio_base64
                }
                ws.send(json.dumps(message))
            
            # ðŸ”‘ æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸï¼Œæäº¤å¹¶è¯·æ±‚å›žå¤
            if speech_status == "speech_end":
                print("ðŸ“¤ æäº¤éŸ³é¢‘ç¼“å†²...")
                
                # 1. æäº¤éŸ³é¢‘
                commit_msg = {
                    "type": "input_audio_buffer.commit"
                }
                ws.send(json.dumps(commit_msg))
                time.sleep(0.1)
                
                # 2. è¯·æ±‚ AI å›žå¤
                response_msg = {
                    "type": "response.create",
                    "response": {
                        "modalities": ["audio", "text"]
                    }
                }
                ws.send(json.dumps(response_msg))
                print("âœ… å·²è¯·æ±‚ AI å›žå¤ï¼Œç­‰å¾…å“åº”...\n")
                
        except queue.Empty:
            # å³ä½¿é˜Ÿåˆ—ç©ºï¼Œä¹Ÿè¦æ£€æŸ¥æ˜¯å¦è¶…æ—¶ç»“æŸè¯­éŸ³
            speech_status = speech_detector.update(False)
            if speech_status == "speech_end":
                print("ðŸ“¤ æäº¤éŸ³é¢‘ç¼“å†²ï¼ˆè¶…æ—¶ï¼‰...")
                commit_msg = {"type": "input_audio_buffer.commit"}
                ws.send(json.dumps(commit_msg))
                time.sleep(0.1)
                
                response_msg = {
                    "type": "response.create",
                    "response": {"modalities": ["audio", "text"]}
                }
                ws.send(json.dumps(response_msg))
                print("âœ… å·²è¯·æ±‚ AI å›žå¤\n")
            continue
        except Exception as e:
            if not stop_event.is_set():
                print(f"âŒ éŸ³é¢‘å‘é€é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()

# --- éŸ³é¢‘æ’­æ”¾çº¿ç¨‹ ---
def play_audio_stream():
    """æ’­æ”¾éŸ³é¢‘æµ"""
    global audio_played_in_response
    
    while not stop_event.is_set():
        try:
            time.sleep(0.05)
            
            with playback_lock:
                if len(audio_playback_buffer) > 0:
                    if not audio_played_in_response:
                        audio_played_in_response = True
                    
                    full_audio = np.concatenate(audio_playback_buffer)
                    audio_playback_buffer.clear()
                    
                    # æ’­æ”¾éŸ³é¢‘
                    try:
                        max_val = np.abs(full_audio).max()
                        if max_val > 0:
                            if max_val < 10000:
                                volume_boost = 10000 / max_val
                                full_audio = (full_audio * volume_boost).astype(np.int16)
                            
                            SPEED_MULTIPLIER = 1.3
                            playback_rate = int(SAMPLE_RATE * SPEED_MULTIPLIER)
                            sd.play(full_audio, samplerate=playback_rate, blocking=True)
                    except Exception as e:
                        print(f"âŒ æ’­æ”¾é”™è¯¯: {e}")
                        
        except Exception as e:
            if not stop_event.is_set():
                print(f"âŒ æ’­æ”¾çº¿ç¨‹é”™è¯¯: {e}")

# --- WebSocket æ¶ˆæ¯å¤„ç† ---
def on_message(ws, message):
    """å¤„ç† WebSocket æ¶ˆæ¯"""
    global ai_is_responding, audio_played_in_response, sync_worker
    
    data = json.loads(message)
    msg_type = data.get("type")
    
    if msg_type in ("session.created", "session.updated", "error", "session.error"):
        print(f"\nðŸ“¡ [{msg_type}] {json.dumps(data, ensure_ascii=False, indent=2)}")
    
    if msg_type in ("session.created", "session.updated"):
        print("âœ… Session Info:", data.get("session", {}).get("id"))
        session_ready.set()
        
    elif msg_type == "conversation.item.input_audio_transcription.completed":
        user_text = data.get("transcript", "")
        if user_text:
            print(f"\nðŸ“ [USER]: {user_text}")
            logger.log_user_input(user_text)
        
    elif msg_type == "response.text.delta":
        text = data.get("text", "")
        if text:
            sys.stdout.write(text)
            sys.stdout.flush()
        
    elif msg_type == "response.text.done":
        sys.stdout.write("\n")
        sys.stdout.flush()
        
    elif msg_type == "response.audio.delta":
        try:
            if not ai_is_responding:
                with ai_response_lock:
                    ai_is_responding = True
                    print("\nðŸ”Š [AI å›žå¤ä¸­] æŒ‰ Enter å¯æ‰“æ–­")
            
            audio_base64 = data.get("delta", "")
            if audio_base64:
                audio_bytes = base64.b64decode(audio_base64)
                audio_np = np.frombuffer(audio_bytes, dtype=np.int16)
                
                with playback_lock:
                    audio_playback_buffer.append(audio_np)
                    
        except Exception as e:
            print(f"âŒ éŸ³é¢‘è§£ç é”™è¯¯: {e}")
    
    elif msg_type == "response.audio_transcript.delta":
        text = data.get("delta", "")
        if text:
            logger.log_assistant_response(text)
    
    elif msg_type == "response.done":
        with ai_response_lock:
            ai_is_responding = False
        
        print("\n" + "="*60)
        print("âœ… å›žå¤å®Œæˆ")
        print("="*60)
        
        # ðŸ”‘ å®žæ—¶åŒæ­¥åˆ° Memobase
        dialogue_data = logger.finalize_turn(synced=False)
        if dialogue_data and sync_worker:
            success = sync_worker.enqueue(dialogue_data)
            if success:
                logger.info("ðŸ“¤ [å®žæ—¶åŒæ­¥] å·²åŠ å…¥åŒæ­¥é˜Ÿåˆ—")
                logger.update_sync_status(dialogue_data['id'], synced=True)
            else:
                logger.warning("âš ï¸  [å®žæ—¶åŒæ­¥] åŠ å…¥é˜Ÿåˆ—å¤±è´¥ï¼Œå°†ç”±å®šæ—¶ä»»åŠ¡å¤„ç†")
        
        audio_played_in_response = False
    
    elif msg_type == "input_audio_buffer.committed":
        print("âœ… éŸ³é¢‘å·²æäº¤")
    
    elif msg_type == "input_audio_buffer.speech_started":
        print("ðŸŽ¤ [æœåŠ¡å™¨æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹]")
    
    elif msg_type == "input_audio_buffer.speech_stopped":
        print("ðŸ”‡ [æœåŠ¡å™¨æ£€æµ‹åˆ°è¯­éŸ³åœæ­¢]")
    
    elif msg_type in ("error", "session.error"):
        error_info = data.get('error', {})
        print(f"âŒ é”™è¯¯: {error_info.get('message', data)}")
    
    elif msg_type == "heartbeat":
        pass
    
    elif msg_type == "rate_limits.updated":
        pass

def on_open(ws):
    """WebSocket è¿žæŽ¥å»ºç«‹"""
    global sync_worker
    
    print("ðŸ”Œ WebSocket connected, configuring session...")
    
    # ðŸ”‘ Client VAD é…ç½® - turn_detection è®¾ç½®ä¸º None
    session_config = {
        "type": "session.update",
        "session": {
            "input_audio_format": "wav",
            "output_audio_format": "pcm",
            "turn_detection": None,  # ðŸ”‘ å…³é”®ï¼šä½¿ç”¨ Client VAD
            "input_audio_transcription": {
                "enabled": True
            },
            "temperature": 0.8,
            "modalities": ["audio", "text"],
            "beta_fields": {
                "chat_mode": "audio",
                "tts_source": "e2e",
                "auto_search": False
            }
        }
    }
    
    print(f"ðŸ“¤ Session config (Client VAD): {json.dumps(session_config, ensure_ascii=False, indent=2)}")
    ws.send(json.dumps(session_config))
    time.sleep(0.5)
    
    # åˆå§‹åŒ–å®žæ—¶åŒæ­¥
    if not sync_worker:
        try:
            sync_worker = create_sync_worker(CURRENT_USER_ID, logger)
            if sync_worker:
                print("âœ… å®žæ—¶åŒæ­¥å·¥ä½œå™¨å·²å¯åŠ¨")
        except Exception as e:
            print(f"âš ï¸  å®žæ—¶åŒæ­¥åˆå§‹åŒ–å¤±è´¥ï¼ˆå°†ä½¿ç”¨å®šæ—¶å¤‡ä»½ï¼‰: {e}")
    
    # å¯åŠ¨éŸ³é¢‘å‘é€çº¿ç¨‹
    threading.Thread(target=send_audio_loop, args=(ws,), daemon=True).start()

def on_close(ws, close_status_code, close_msg):
    """WebSocket è¿žæŽ¥å…³é—­"""
    if not stop_event.is_set():
        print(f"ðŸ”Œ è¿žæŽ¥å·²å…³é—­: code={close_status_code}, msg={close_msg}")

def on_error(ws, error):
    """WebSocket é”™è¯¯"""
    print(f"âŒ WebSocket é”™è¯¯: {error}")

# --- ä¸»ç¨‹åºå‡½æ•° ---
def main():
    """ä¸»ç¨‹åºå…¥å£"""
    global sync_worker
    
    if not API_KEY:
        print("âŒ è¯·è®¾ç½® ZHIPU_API_KEY çŽ¯å¢ƒå˜é‡")
        sys.exit(1)
    
    print("\n" + "="*80)
    print("    GLM-Realtime è¯­éŸ³åŠ©æ‰‹ (Client VAD æ¨¡å¼)")
    print("="*80)
    print("ðŸ“Œ æ¨¡å¼è¯´æ˜Žï¼š")
    print("   â€¢ Client VAD - å®¢æˆ·ç«¯æŽ§åˆ¶è¯­éŸ³æ£€æµ‹")
    print("   â€¢ è‡ªåŠ¨æ£€æµ‹è¯­éŸ³ç»“æŸï¼ˆé™éŸ³ 1.5 ç§’ï¼‰å¹¶è¯·æ±‚ AI å›žå¤")
    print("   â€¢ æŒ‰ Enter å¯æ‰“æ–­ AI å›žå¤")
    print("   â€¢ Ctrl+C é€€å‡ºç¨‹åº")
    print("="*80 + "\n")
    
    # ç”Ÿæˆ JWT Token
    try:
        AUTH_TOKEN = generate_jwt_token(API_KEY)
        print("âœ… JWT Token ç”ŸæˆæˆåŠŸ\n")
    except Exception as e:
        print(f"âŒ JWT Token ç”Ÿæˆå¤±è´¥: {e}")
        sys.exit(1)
    
    # å¯åŠ¨ TTS å·¥ä½œçº¿ç¨‹
    tts_thread = threading.Thread(target=tts_worker_thread, daemon=True)
    tts_thread.start()
    
    # å¯åŠ¨éŸ³é¢‘æ’­æ”¾çº¿ç¨‹
    playback_thread = threading.Thread(target=play_audio_stream, daemon=True)
    playback_thread.start()
    
    # åˆ›å»º WebSocket è¿žæŽ¥
    websocket.enableTrace(False)
    
    ws = websocket.WebSocketApp(
        WS_URL,
        header=[f"Authorization: Bearer {AUTH_TOKEN}"],
        on_message=on_message,
        on_open=on_open,
        on_close=on_close,
        on_error=on_error
    )
    
    # å¯åŠ¨å½•éŸ³
    try:
        with sd.InputStream(
            channels=1,
            samplerate=SAMPLE_RATE,
            dtype='int16',
            blocksize=CHUNK,
            callback=audio_callback
        ):
            print("ðŸŽ™ï¸  å½•éŸ³è®¾å¤‡å·²å¯åŠ¨\n")
            ws.run_forever()
            
    except KeyboardInterrupt:
        print("\n\nðŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        stop_event.set()
        
        # åœæ­¢åŒæ­¥å·¥ä½œå™¨
        if sync_worker:
            print("\nðŸ›‘ æ­£åœ¨åœæ­¢å®žæ—¶åŒæ­¥...")
            sync_worker.stop(timeout=5)
        
        # åœæ­¢éŸ³é¢‘
        sd.stop()
        
        # åœæ­¢é”®ç›˜ç›‘å¬
        keyboard_listener.stop()
        
        print("\nç¨‹åºå·²é€€å‡ºã€‚")

# --- ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    main()

